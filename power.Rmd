---
title: "Replication Crisis + Power"
author: "Dan Birman"
date: "September 20, 2016"
output: html_document
---

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
stroop_data = read.csv("https://github.com/dbirman/bootcamp-data/raw/master/stroop.csv")
```

Replication Crisis + Power
------------

You have almost certainly heard about the replication crisis and how medicine, neuroscience, psychology, and many other fields are plagued with studies that don't replicate. What's going on? One problem is that the average study in our field is designed with a low statistical power. This means that even if the study is looking for a real effect, it's unlikely to find it--and it also inflates the likelihood that if you find an effect it's actually a false positive. We're going to work through the idea of power in this worksheet and write some functions to simulate it.

For our example we'll focus on the same t-test you performed earlier. Instead of asking whether two sets of data are different, or not, let's now ask a more abstract question: how often do we expect to end up with data that we *think* is different, given that the effect underneath is present or absent? 

Look at the plot below. In green we will plot the reaction times for congruent trials, and in red we will plot reaction times for incongruent trials.

```{r}
ggplot(stroop_data,aes(x=rt,fill=condition)) +
  geom_histogram()
```

Note that there's a tiny bit of cleaning up to do here, because there shouldn't be any data at zero... These are probably people who were holding the response key down as the trial started.

```{r}
# your code here, use dplyr/tidyr to "filter" out trials where the rt was zero or negative
```

Now we'll do this again but summarising the data under the assumption of normality. The data is obviously non-normal so this is *not a good thing to do!!*. Fortunately, this demo is all about bootstrapping, and we won't make any assumptions about normality in our actual analysis. So for the purpose of exposition this step simplifies the visualization as we play around with the concept of power.

```{r}
cond_means = stroop_data %>% group_by(condition) %>% summarise(mu=mean(rt),sd=sd(rt))

ggplot(data.frame(x = c(0, 1500)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = cond_means$mu[1], sd = cond_means$sd[1]), color='#b2ffb2', size=1) +
  stat_function(fun = dnorm, args=list(mean=cond_means$mu[2],sd=cond_means$sd[2]), color='#ffb2b2',size=1) +
  geom_vline(aes(xintercept=0),color='black',size=1,linetype='dashed') +
  theme_bw()
```

Cool. So what's the idea behind power? Well--first of all you need to recognize that the data we're plotting here is a *sample*. It's only one possible sample, we could in fact have gotten a very different result. If we took many samples, the mean RT for each group would shift around slightly. When we compute the mean and variance of our sample it gives us an idea of what the *true* mean is (if it exists). So let's imagine for a moment that the true mean RT for congruent trials is actually 500 ms, and incongruent trials is 1000 ms, as follows. Before plotting you need to take a moment to think about what the variance means here--this isn't the variance across subjects or across trials, it's the variance across samples. I've set it to be 100 (arbitrarily).

```{r}
ggplot(data.frame(x = c(0, 1500)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = 500, sd = 100), color='#b2ffb2', size=1) +
  stat_function(fun = dnorm, args=list(mean=1000, sd=100), color='#ffb2b2',size=1) +
  geom_vline(aes(xintercept=0),color='black',size=1,linetype='dashed') +
  theme_bw()
```

Okay. Now we can talk about power. What's the probability that if these are the distributions of *possible* sample means, we would think they were different? For a first pass let's imagine that these sample mean distributions are normal, which simplifies sampling from them because there is an existing random normal sampling function `rnorm`. We're going to write a function which will grab two samples, compute an effect size (difference between the samples), and return that.

```{r}
get2samples = function(mu1,sd1,mu2,sd2) {
  # your code here
  # get a sample using rnorm for each distribution
  # compute the effect size (difference between samples)
  # return the effect
}

simulation = data.frame(effect=replicate(10,get2samples(500,100,1000,100)))
```

All right. It should be obvious if you look at the mean: `mean(simulation$effect)` that it basically always comes out positive. That means we would find that the incongruent RT is longer than the congruent RT, if the true effects were 500 ms apart. Note we haven't done any computation of p-values yet, we're just looking right now at how often the effect would go in the direction we expect, based on how we set up the problem. Let's check that for this dataset by making a REALLY large simulation.

```{r}
simulation = data.frame(effect=replicate(10000,get2samples(500,100,1000,100)))
# your code here: write a ggplot line that uses the simulation data.frame to plot a histogram of the effects
```

In this simulation, the effect would always be large and positive. Let's now refine our simulation based on the data we have access to. In computing power there are a few things we might want to know:

 - How often will we get the direction of the effect correct?
 - How often will we consider the effect statistically "significant" when it exists (hit), or when it does not exist (false positive)
 
Etc...

Let's look at just those two. What we're going to do is take the actual data we have and collect samples of increasing sizes. If we only had one trial, from one subject, what would be the likelihood of the above events coming out the way we expect? If we had 23000 trials, as we do here, what would be the likelihood of these events? Let's start with the direction of the effect.

We'll write a function and a helper that, given a sampling size, computes 10,000 repeats of getting a congruent and an incongruent sample, getting their means, and checking the direction. It will return the percentage of times that the direction went the right way for that sample size.

```{r}
# separate out the RT data
stroop_con_rts = stroop_data %>% filter(condition=='congruent') %>% select(rt) %>% .$rt
stroop_inc_rts = stroop_data %>% filter(condition=='incongruent') %>% select(rt) %>% .$rt

dirHelper = function(n,con_rts,inc_rts) {
  # your code here, get a sample from each group
  # compute the difference in samples
  # if the difference is in the right direction, return 1, else 0
}

getDirectionPercentage = function(n,con_rts,inc_rts,reps) {
  repeats = replicate(reps,dirHelper(n,con_rts,inc_rts))
  return(sum(repeats)/reps)
}
```

Let's test this function out for a sample with one trial:

```{r}
getDirectionPercentage(1,stroop_con_rts,stroop_inc_rts,10000)
```

Cool, so if we only had two trials total, one congruent and one incongruent, we would expect to find that the incongruent one would be slower 66.6% of the time. Let's plot how this percentage increases as a function of sample size for this dataset.

```{r}
sample_size = 1:100
perc = c()
for (i in sample_size) {
  perc[i] = getDirectionPercentage(i,stroop_con_rts,stroop_inc_rts,10000)
}
```

```{r}
# we separate into two code blocks so you can run the simulation above once, and then here you can plot it
data = data.frame(samplesize=sample_size,percentage=perc)

ggplot(data,aes(x=samplesize,y=percentage)) +
  geom_point()
```

By the way, at this point it's good to mention that a lot of this can be parametrized using studied measures of effect size such as d-prime. D-prime is calculated as $(mu_{1}-mu_{2})/sd$, for our data set we can calculate it as follows:

```{r}
mu_con = mean(stroop_con_rts)
mu_inc = mean(stroop_inc_rts)
joint_sd = sd(stroop_data$rt)
dprime = (mu_inc-mu_con)/joint_sd
```

A dprime of 0.5 is a "medium" effect. Note that we're computing effect size here using the variability across trials, i.e. as a "within-subject" effect: in other words for a given person, how much of a difference is there in their reaction time for congruent vs. incongruent. We could also use the variability across subjects if we were interested in talking about how subjects differed "between-subjects".

Now finally, we can also compute the expected proportion of results that not only go in the right direction but are "statistically significant", i.e. the size of the effect in a given sample is sufficiently large. Often in psychology a cutoff is set at .05. In other words: is this effect large enough that we only expect to see it 5% of the time in an imagined world where this effect doesn't exist? If we get this far in the tutorial day, we will write this code individually!
